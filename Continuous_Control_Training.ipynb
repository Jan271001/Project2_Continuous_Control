{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2deb3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:matplotlib.font_manager:Generating new fontManager, this may take some time...\n",
      "INFO:matplotlib.font_manager:Failed to extract font properties from /usr/share/fonts/truetype/noto/NotoColorEmoji.ttf: In FT2Font: Can not load face.  Unknown file format.\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from agent import Agent\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f4ff6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Env_1_Agent/Reacher_Linux/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f68a52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "278ac442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dae3641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nenv_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \\nstates = env_info.vector_observations                  # get the current state (for each agent)\\nscores = np.zeros(num_agents)                          # initialize the score (for each agent)\\nwhile True:\\n    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\\n    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\\n    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\\n    next_states = env_info.vector_observations         # get next state (for each agent)\\n    rewards = env_info.rewards                         # get reward (for each agent)\\n    dones = env_info.local_done                        # see if episode finished\\n    scores += env_info.rewards                         # update the score (for each agent)\\n    states = next_states                               # roll over states to next time step\\n    if np.any(dones):                                  # exit loop if episode finished\\n        break\\nprint('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce6076e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition hyperparameters and trainingsconditions\n",
    "n_episodes = 500\n",
    "max_eps= 1.0\n",
    "min_eps = 0.01\n",
    "eps_decay = 0.995\n",
    "training_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80b34480",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 Score:[0.19]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize Feed-forward DNNs for Actor and Critic models. \n",
    "agent = Agent(state_size, action_size, random_seed=0)\n",
    "\n",
    "#train the agent\n",
    "def ddpg_train(n_episodes, eps_start, eps_end, eps_decay):\n",
    "\n",
    "    #list containing scores from each episode\n",
    "    scores = []\n",
    "    #last 100 scores\n",
    "    scores_window = deque(maxlen=100)\n",
    "    #initialize epsilon\n",
    "    eps = eps_start\n",
    "    \n",
    "    for episode in range(n_episodes): \n",
    "        # Reset the enviroment\n",
    "        env_info = env.reset(train_mode=training_mode)[brain_name] \n",
    "        cur_state = env_info.vector_observations\n",
    "        score= np.zeros(num_agents)\n",
    "        \n",
    "        while True:\n",
    "      \n",
    "            # Predict the action for the current state.\n",
    "            # Achtung Epsilon entspricht noch nicht dem wahren Epsilon\n",
    "            if random.random() < eps:\n",
    "                # Choose random action\n",
    "                action = np.random.randn(num_agents, action_size)\n",
    "                action = np.clip(action, -1, 1)\n",
    "            else:\n",
    "                # Choose best action for given network\n",
    "                action = agent.act(cur_state, add_noise = False)\n",
    "        \n",
    "            # Action is performed and new state, reward, info are received. \n",
    "            env_info = env.step(action)[brain_name]\n",
    "        \n",
    "            # get next state \n",
    "            next_state = env_info.vector_observations\n",
    "            \n",
    "            # see if episode is finished\n",
    "            done = env_info.local_done\n",
    "            \n",
    "            # get reward\n",
    "            reward = env_info.rewards \n",
    "            \n",
    "            # current state, action, reward, new state are stored in the experience replay\n",
    "            agent.step(cur_state, action, reward, next_state, done)\n",
    "        \n",
    "            # roll over new state\n",
    "            cur_state = next_state\n",
    "            \n",
    "            #add reward to score\n",
    "            score += reward \n",
    "            \n",
    "            if np.any(done):\n",
    "                break\n",
    "        \n",
    "        print(\"Episode:\" + str(episode) + \" Score:\" + str(score))\n",
    "        \n",
    "        #decrease epsilon\n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "        \n",
    "        #save most recent score\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        \n",
    "        # save model weights and print mean score over last 100 episodes\n",
    "        if (episode+1) % 100 == 0:\n",
    "            #torch.save(agent.qnetwork_local.state_dict(), 'checkpoint_Actor.pth')\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode+1, np.mean(scores_window)))\n",
    "    \n",
    "    return scores\n",
    "\n",
    "scores = ddpg_train(n_episodes, max_eps, min_eps, eps_decay)\n",
    "\n",
    "#plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b3cf90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
