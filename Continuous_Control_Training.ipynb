{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2deb3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from agent import Agent\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f4ff6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnityTimeOutException",
     "evalue": "The Unity environment took too long to respond. Make sure that :\n\t The environment does not need user interaction to launch\n\t The Academy and the External Brain(s) are attached to objects in the Scene\n\t The environment and the Python interface have compatible versions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnityTimeOutException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b6f828017f2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnityEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Env_20_Agents/Reacher_Linux/Reacher.x86_64'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_graphics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_name, worker_id, base_port, curriculum, seed, docker_training, no_graphics)\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0maca_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_academy_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl_init_parameters_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnityTimeOutException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36msend_academy_parameters\u001b[0;34m(self, init_parameters)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnityInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_initialization_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_initialization_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrap_unity_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrl_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnityRLInput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnityOutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/unityagents/rpc_communicator.py\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             raise UnityTimeOutException(\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0;34m\"The Unity environment took too long to respond. Make sure that :\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0;34m\"\\t The environment does not need user interaction to launch\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;34m\"\\t The Academy and the External Brain(s) are attached to objects in the Scene\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnityTimeOutException\u001b[0m: The Unity environment took too long to respond. Make sure that :\n\t The environment does not need user interaction to launch\n\t The Academy and the External Brain(s) are attached to objects in the Scene\n\t The environment and the Python interface have compatible versions."
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Env_20_Agents/Reacher_Linux/Reacher.x86_64', no_graphics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278ac442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6076e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition hyperparameters and trainingsconditions\n",
    "n_episodes = 2\n",
    "max_noice= 1.0\n",
    "noice_decay = 0.995\n",
    "training_mode = True\n",
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b34480",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize Feed-forward DNNs for Actor and Critic models. \n",
    "agent = Agent(state_size, action_size, random_seed, max_noice, noice_decay)\n",
    "\n",
    "#train the agent\n",
    "def ddpg_train(n_episodes):\n",
    "    #list containing scores from each episode\n",
    "    scores = []\n",
    "    #list containing means over last 100 episodes\n",
    "    means = []\n",
    "    #last 100 scores\n",
    "    scores_window = deque(maxlen = 100)\n",
    "    # messure time for achieving a mean score over 30\n",
    "    start_time = time.time()\n",
    "    for episode in range(n_episodes):\n",
    "        # messure time for one episode\n",
    "        start_episode = time.time()\n",
    "        # Reset the enviroment\n",
    "        env_info = env.reset(train_mode=training_mode)[brain_name] \n",
    "        cur_states = env_info.vector_observations\n",
    "        score = np.zeros(num_agents)\n",
    "        # initialize timestep\n",
    "        timestep = 0\n",
    "        # reset noise\n",
    "        agent.reset()\n",
    "        while True:\n",
    "            # Choose best action for given network\n",
    "            actions = agent.act(cur_states, add_noise = True)\n",
    "            # Action is performed and new state, reward, info are received. \n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            # get next state \n",
    "            next_states = env_info.vector_observations\n",
    "            # see if episode is finished\n",
    "            dones = env_info.local_done\n",
    "            # get reward\n",
    "            rewards = env_info.rewards\n",
    "            # save experience to replay buffer, perform learning step at defined interval\"\n",
    "            for cur_state, action, reward, next_state, done in zip(cur_states, \n",
    "                                                                   actions, \n",
    "                                                                   rewards, \n",
    "                                                                   next_states, \n",
    "                                                                   dones):\n",
    "                # current state, action, reward, new state are stored in the experience replay\"\n",
    "                agent.step(cur_state, action, reward, next_state, done, timestep)\n",
    "            # roll over new state\n",
    "            cur_states = next_states      \n",
    "            #add reward to score\n",
    "            score += rewards\n",
    "            # count timestep\n",
    "            timestep+=1\n",
    "            \n",
    "            if np.any(dones):\n",
    "                break\n",
    "        \n",
    "        # calculate time\n",
    "        time_episode = time.time() - start_episode\n",
    "        time_entire = time.time() - start_time\n",
    "        #save most recent score\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        mean = np.mean(scores_window)\n",
    "        means.append(mean)\n",
    "        print(\"Episode:\" + str(episode) + \" Score:\" + str(np.mean(score)) + \n",
    "              \" Mean Score(last 100 episodes):\" + str(np.mean(scores_window)) +\n",
    "              \" Duration episode:\" + time.strftime('%Mm%Ss', time.gmtime(time_episode)) + \n",
    "              \" Duration training:\" + time.strftime('%Dd%Hh%Mm%Ss', time.gmtime(time_entire)))\n",
    "        \n",
    "        # save model weights\n",
    "        if (episode+1) % 10 == 0 or np.mean(scores_window) >= 30:\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_Actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_Critic.pth')\n",
    "            if np.mean(scores_window) >= 30:\n",
    "                print(\"Environment solved in \" + str(episode) + \" episodes. Mean score over all 20 agents \" +\n",
    "                      str(np.mean(scores_window)) + \" for the last 100 episodes\")\n",
    "    \n",
    "    return scores, means\n",
    "\n",
    "scores, means = ddpg_train(n_episodes)\n",
    "\n",
    "#plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores, label = \"scores\")\n",
    "plt.plot(np.arange(len(means)), means, label = \"means\")\n",
    "plt.axhline(y=30, color='r', linestyle='--', label=\"target\")\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3cf90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
